{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is created especially for Sber x Skoltech hackathon for risk modeling task at 29-31 October of 2021. It suppose to train a model for the probability of first payment default calculations. Notebook load train data, fit model, load test set, and predict probabilities of first payment default for the test set. Results are in form of 'submission.csv' with predictions, pickled fitted model 'model.pkl' and pickled features 'features.pkl'. The autors: Oleg Nikolaev, Nikita Kuznetsov, Anton Nevskii\n",
    "\n",
    "Contact mail: oleg.nikolaev@skoltech.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-31T05:32:47.653857Z",
     "iopub.status.busy": "2021-10-31T05:32:47.653247Z",
     "iopub.status.idle": "2021-10-31T05:32:48.647411Z",
     "shell.execute_reply": "2021-10-31T05:32:48.646406Z",
     "shell.execute_reply.started": "2021-10-31T05:32:47.653771Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and modify train data\n",
    "We load only one part because the model is not receiving any visible improvement as we load and concatenate two parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T05:32:48.649398Z",
     "iopub.status.busy": "2021-10-31T05:32:48.649163Z",
     "iopub.status.idle": "2021-10-31T05:33:23.883896Z",
     "shell.execute_reply": "2021-10-31T05:33:23.882999Z",
     "shell.execute_reply.started": "2021-10-31T05:32:48.649371Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "data_p1_link = '/kaggle/input/risk-management-uiim/train_part1.pkl'\n",
    "data_p2_link = '/kaggle/input/risk-management-uiim/train_part2.pkl'\n",
    "data_test_link = '/kaggle/input/risk-management-uiim/test_data.pkl'\n",
    "submission_link = '/kaggle/input/risk-management-uiim/submission.csv'\n",
    "\n",
    "df = pd.read_pickle(data_p2_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort data by time as we conna use such characteristic as 'year', and we will split our train set on train and validation **without** shafle - so our model will be fitted on 'old' dates and will predict on latest dates. By this we can predict future trend, for next years (this is done thanks to comment from one of the Sber experts about that fact that simply 'year' is not informative, as global market has economic waves) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T05:33:23.885505Z",
     "iopub.status.busy": "2021-10-31T05:33:23.885252Z",
     "iopub.status.idle": "2021-10-31T05:33:31.980305Z",
     "shell.execute_reply": "2021-10-31T05:33:31.979342Z",
     "shell.execute_reply.started": "2021-10-31T05:33:23.885475Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=['REPORT_DT'],axis=0, ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T05:33:31.98262Z",
     "iopub.status.busy": "2021-10-31T05:33:31.981685Z",
     "iopub.status.idle": "2021-10-31T05:33:32.595928Z",
     "shell.execute_reply": "2021-10-31T05:33:32.595093Z",
     "shell.execute_reply.started": "2021-10-31T05:33:31.982578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Categorial features with str format\n",
    "cat_features=['x_618','x_628','x_13']\n",
    "# Dates\n",
    "date=['REPORT_DT','x_9']\n",
    "# Numerical featues which have no big ammount of different numbers, so it can be can be transfer to numerical\n",
    "num_features_pse_cat=['x_0','x_2','x_3']\n",
    "# Numerical featues\n",
    "num_features=['x_4','x_5','x_7','x_317','x_286','x_292','x_189','x_321','x_291','x_63','x_85','x_124','x_421','x_183','x_111','x_100','x_330']\n",
    "\n",
    "\n",
    "# To add second part of train data \n",
    "#particulary its not needed - no huge diffference\n",
    "\n",
    "# data_k2=data[[*cat_features, *date, *num_features_pse_cat, *num_features, 'TARGET']]\n",
    "# data_p1 = pd.read_pickle(data_p1_link)\n",
    "# data_k1=data_p1[[*cat_features, *date, *num_features_pse_cat, *num_features, 'TARGET']]\n",
    "# del data_p1\n",
    "# data=pd.concat([data_k1,data_k2],axis=0)\n",
    "\n",
    "df=df[[*cat_features, *date, *num_features_pse_cat, *num_features, 'TARGET']]\n",
    "\n",
    "d1=pd.DatetimeIndex(df['REPORT_DT'].values.astype('<M8[M]'))\n",
    "d0=pd.DatetimeIndex(df['x_9'].values.astype('<M8[M]'))\n",
    "delta = d1 - d0\n",
    "df['delta']=delta.days\n",
    "df['year']=d1.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill NaN values in columns with median value for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T05:33:32.598175Z",
     "iopub.status.busy": "2021-10-31T05:33:32.597932Z",
     "iopub.status.idle": "2021-10-31T05:33:33.535684Z",
     "shell.execute_reply": "2021-10-31T05:33:33.534783Z",
     "shell.execute_reply.started": "2021-10-31T05:33:32.598147Z"
    }
   },
   "outputs": [],
   "source": [
    "df.fillna(df.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode categorical three features with lables to make them useable during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T05:33:33.537193Z",
     "iopub.status.busy": "2021-10-31T05:33:33.53695Z",
     "iopub.status.idle": "2021-10-31T05:33:36.545521Z",
     "shell.execute_reply": "2021-10-31T05:33:36.544644Z",
     "shell.execute_reply.started": "2021-10-31T05:33:33.537164Z"
    }
   },
   "outputs": [],
   "source": [
    "cleanup_nums = {\n",
    "                'x_13':      {\"1\": 0,\n",
    "                             \"19\": 7,\n",
    "                             \"2\": 2,\n",
    "                             \"3\": 3,\n",
    "                             \"4\": 4,\n",
    "                             \"5\": 5,\n",
    "                             \"9\": 6,\n",
    "                             \"None\":8},\n",
    "                 'x_618':    {\"Приобретение\": 0,\n",
    "                             \"Инвестирование\": 1,\n",
    "                             \"Нецелевой кредит под залог недвижимости\": 2,\n",
    "                             \"Рефинансирование\": 3,\n",
    "                             \"Индивидуальное строительство\": 4},\n",
    "                 'x_628':    {\"ЗП\": 0,\n",
    "                             \"Сотрудники\": 1,\n",
    "                             \"Улица\": 2},    \n",
    "                }\n",
    "\n",
    "df.drop(['REPORT_DT','x_9'], axis=1,inplace=True)\n",
    "df = df.replace(cleanup_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training dataset into two parts using Sklearn function. Since there is no need to split randomly (because of timeline) shuffle is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T05:33:36.546991Z",
     "iopub.status.busy": "2021-10-31T05:33:36.546741Z",
     "iopub.status.idle": "2021-10-31T05:33:36.778955Z",
     "shell.execute_reply": "2021-10-31T05:33:36.778094Z",
     "shell.execute_reply.started": "2021-10-31T05:33:36.546962Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('TARGET', axis=1), df['TARGET'], test_size=0.20, random_state=SEED, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Logistic Regression model from Sklearn library with fixed random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T05:33:36.780347Z",
     "iopub.status.busy": "2021-10-31T05:33:36.780133Z",
     "iopub.status.idle": "2021-10-31T05:35:02.689906Z",
     "shell.execute_reply": "2021-10-31T05:35:02.689043Z",
     "shell.execute_reply.started": "2021-10-31T05:33:36.780323Z"
    }
   },
   "outputs": [],
   "source": [
    "clf =LogisticRegression(solver='newton-cg',random_state=SEED, penalty='l2')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment code below in order to calculate roc_auc metric for train and test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T05:35:02.691838Z",
     "iopub.status.busy": "2021-10-31T05:35:02.69131Z",
     "iopub.status.idle": "2021-10-31T05:35:03.171474Z",
     "shell.execute_reply": "2021-10-31T05:35:03.170569Z",
     "shell.execute_reply.started": "2021-10-31T05:35:02.691794Z"
    }
   },
   "outputs": [],
   "source": [
    "# To see accuracy score\n",
    "\n",
    "print('TRAIN SCORE', roc_auc_score(y_train,clf.predict_proba(X_train)[:,1]))\n",
    "print('TEST SCORE', roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the code below in order to obtain trained weights of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T05:35:03.174061Z",
     "iopub.status.busy": "2021-10-31T05:35:03.173332Z",
     "iopub.status.idle": "2021-10-31T05:35:03.709855Z",
     "shell.execute_reply": "2021-10-31T05:35:03.709035Z",
     "shell.execute_reply.started": "2021-10-31T05:35:03.174007Z"
    }
   },
   "outputs": [],
   "source": [
    "# To see weights of the model\n",
    "\n",
    "pd.options.display.max_columns=3\n",
    "X_coef=np.transpose(X_train)\n",
    "X_coef['coef']=clf.coef_[0]\n",
    "abs(X_coef).sort_values('coef', axis=0, ascending=False)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete previously used dataframe to clean RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T05:35:03.711284Z",
     "iopub.status.busy": "2021-10-31T05:35:03.711048Z",
     "iopub.status.idle": "2021-10-31T05:35:03.717095Z",
     "shell.execute_reply": "2021-10-31T05:35:03.716281Z",
     "shell.execute_reply.started": "2021-10-31T05:35:03.711256Z"
    }
   },
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download test dataset and submission template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T05:35:03.719409Z",
     "iopub.status.busy": "2021-10-31T05:35:03.718837Z",
     "iopub.status.idle": "2021-10-31T05:35:37.640168Z",
     "shell.execute_reply": "2021-10-31T05:35:37.639132Z",
     "shell.execute_reply.started": "2021-10-31T05:35:03.719361Z"
    }
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_pickle(data_test_link)\n",
    "submission = pd.read_csv(submission_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the same manipulations with test dataset as it was with train dataset: generate specific features, fill in the empty values, encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-31T05:35:37.642288Z",
     "iopub.status.busy": "2021-10-31T05:35:37.64134Z"
    }
   },
   "outputs": [],
   "source": [
    "df=data_test[[*cat_features, *date, *num_features_pse_cat, *num_features]]\n",
    "d1=pd.DatetimeIndex(df['REPORT_DT'].values.astype('<M8[M]'))\n",
    "d0=pd.DatetimeIndex(df['x_9'].values.astype('<M8[M]'))\n",
    "delta = d1 - d0\n",
    "df['delta']=delta.days\n",
    "df['year']=d1.year\n",
    "df.fillna(df.median(), inplace=True)\n",
    "cleanup_nums = {\n",
    "                'x_13':      {\"1\": 0,\n",
    "                             \"19\": 7,\n",
    "                             \"2\": 2,\n",
    "                             \"3\": 3,\n",
    "                             \"4\": 4,\n",
    "                             \"5\": 5,\n",
    "                             \"9\": 6,\n",
    "                             \"None\":8},\n",
    "                 'x_618':    {\"Приобретение\": 0,\n",
    "                             \"Инвестирование\": 1,\n",
    "                             \"Нецелевой кредит под залог недвижимости\": 2,\n",
    "                             \"Рефинансирование\": 3,\n",
    "                             \"Индивидуальное строительство\": 4},\n",
    "                 'x_628':    {\"ЗП\": 0,\n",
    "                             \"Сотрудники\": 1,\n",
    "                             \"Улица\": 2},    \n",
    "                }\n",
    "\n",
    "df.drop(['REPORT_DT','x_9'], axis=1,inplace=True)\n",
    "df = df.replace(cleanup_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate probabilities for test dataset and form .csv file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Probability'] = clf.predict_proba(df)[:,1]\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "FEATURES=df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as files:\n",
    "    pickle.dump(clf, files)\n",
    "with open('features.pkl', 'wb') as files:\n",
    "    pickle.dump(FEATURES, files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
